{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbaeumel/transformer-digit-arithmetic/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIztQfmuPlzv"
      },
      "source": [
        "Get the data from the repo -> anonymize!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeyBSXiAPlTn"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/tbaeumel/transformer-digit-arithmetic.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1j_pJjQPSh1"
      },
      "outputs": [],
      "source": [
        "%cd transformer-digit-arithmetic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi4rRDHKaLOi"
      },
      "source": [
        "Get pyvene\n",
        "Olmo2 was not in pyvene - i wrote an extension so that olmo2 can be an intervenable model.\n",
        " has to be anonymized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijicRAr8pFiK"
      },
      "outputs": [],
      "source": [
        "# pip uninstall -y pyvene  # pyvene has to be uninstalled if a standard version is currently installed\n",
        "# pip install pyvene\n",
        "!pip install git+https://github.com/tbaeumel/pyvene"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQSGxa7hQcKu"
      },
      "outputs": [],
      "source": [
        "import pyvene as pv\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "import sys\n",
        "import torch.nn.functional as F\n",
        "from pyvene import embed_to_distrib, top_vals, format_token\n",
        "from pyvene import (\n",
        "    ZeroIntervention,\n",
        "    IntervenableModel,\n",
        "    VanillaIntervention, Intervention,\n",
        "    RepresentationConfig,\n",
        "    IntervenableConfig,\n",
        "    ConstantSourceIntervention,\n",
        "    LocalistRepresentationIntervention\n",
        ")\n",
        "from huggingface_hub import login\n",
        "import re\n",
        "import numpy\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "import transformers\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93z_4vN6QivG"
      },
      "source": [
        "## Load model\n",
        "\n",
        "Llama 70B wont work on colab\n",
        "\n",
        "llama 8b and olmo 2 7b will work on l4 or a100\n",
        "\n",
        "people have to login with their huggingface key to get access to the llama models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DtGz_kwgQjxm"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'login' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Login to Huggingface to get access to model parameters\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Paste your token here\u001b[39;00m\n\u001b[1;32m      3\u001b[0m HugginFace_Token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<replace_with_your_token>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mlogin\u001b[49m(HugginFace_Token)\n\u001b[1;32m      6\u001b[0m models \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama-3-8B\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama-3-70B\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-70B\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOlmo-2-7B\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallenai/OLMo-2-1124-7B\u001b[39m\u001b[38;5;124m\"\u001b[39m }\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# choose your model\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'login' is not defined"
          ]
        }
      ],
      "source": [
        "# Login to Huggingface to get access to model parameters\n",
        "# Paste your token here\n",
        "HugginFace_Token = \"<replace_with_your_token>\"\n",
        "login(HugginFace_Token)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCBzhTaDQx8O"
      },
      "outputs": [],
      "source": [
        "models = {\"Llama-3-8B\": \"meta-llama/Meta-Llama-3-8B\", \"Llama-3-70B\": \"meta-llama/Meta-Llama-3-70B\", \"Olmo-2-7B\": \"allenai/OLMo-2-1124-7B\" }\n",
        "# choose your model\n",
        "model_name = \"Llama-3-8B\" # Llama-3-70B # Olmo-2-7B\n",
        "\n",
        "# Set parameters\n",
        "params = {\n",
        "    'model_name': models[model_name],\n",
        "    'device': \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "}\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(params['model_name'])\n",
        "model = AutoModelForCausalLM.from_pretrained(params['model_name'], torch_dtype=torch.float16).to(params['device'])\n",
        "\n",
        "# Confirm device\n",
        "print(f\"Using device: {params['device']}\")\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  print(\"Using GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbk3s-9eQ8OR"
      },
      "source": [
        "# Digit circuit Intervention\n",
        "\n",
        "Choose:\n",
        "- thresholds (pre computed using Fisher scores)\n",
        "- digit position (hundredth,tenth,unit)\n",
        "- task (operator) (+,-)\n",
        "- operand (which intervention dataset) (op1 + op2 = ..) two variation either fix op1 or op2\n",
        "\n",
        "this can be changed to standard values for the thresholds and layers (see paper Section X), so that based on model and digti position the best threshold is chosen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0xUTzMfLQ9tP"
      },
      "outputs": [],
      "source": [
        "# labels\n",
        "label = \"hundredth\" # \"tenth\" # \"unit\"\n",
        "# task (operator)\n",
        "task = \"addition\" # \"subtraction\"\n",
        "# which oeprand\n",
        "operand = \"op1\" # \"op2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HwIGZ-L0RFNW"
      },
      "outputs": [],
      "source": [
        "# Load the JSON file\n",
        "input_file = f\"Intervention_Data/intervention_data_{task}_{operand}.json\"\n",
        "with open(input_file, \"r\") as f:\n",
        "    data = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wubB5XCNRHBO"
      },
      "source": [
        "all of this should be stored in some sort of data look up file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\"Llama-3-8B\": \"meta-llama/Meta-Llama-3-8B\", \"Llama-3-70B\": \"meta-llama/Meta-Llama-3-70B\", \"Olmo-2-7B\": \"allenai/OLMo-2-1124-7B\" }\n",
        "import json\n",
        "model_name = \"Llama-3-8B\" # Llama-3-70B # Olmo-2-7B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wqRyI89ZRQ2O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
            "0.8\n"
          ]
        }
      ],
      "source": [
        "# layer sets to intervene on based on model, task, and operand\n",
        "# TODO do this elegantly\n",
        "\n",
        "key = f\"{model_name}_{task}_{operand}\"\n",
        "\n",
        "layer_sets = {\n",
        "    \"Llama-3-8B_addition_op1\": list(range(16,25)),\n",
        "    \"Llama-3-8B_addition_op2\": list(range(15,24)),\n",
        "    \"Llama-3-8B_subtraction_op1\": list(range(16,29)),\n",
        "    \"Llama-3-8B_subtraction_op2\": list(range(15,29)),\n",
        "    \"Llama-3-70B_addition_op1\": list(range(39,57)),\n",
        "    \"Llama-3-70B_addition_op2\": list(range(39,57)),\n",
        "    \"Llama-3-70B_subtraction_op1\": list(range(39,59)),\n",
        "    \"Llama-3-70B_subtraction_op2\": list(range(39,59)),\n",
        "    \"Olmo-2-7B_addition_op1\": list(range(17,31)),\n",
        "    \"Olmo-2-7B_addition_op2\": list(range(17,31)),\n",
        "    \"Olmo-2-7B_subtraction_op1\": list(range(19,28)),\n",
        "    \"Olmo-2-7B_subtraction_op2\": list(range(17,28)),\n",
        "    }\n",
        "\n",
        "layer_set = layer_sets[key]\n",
        "\n",
        "# TODO add llama 70 when results are there (best threshold)\n",
        "key = key+\"_\"+label\n",
        "threshold_map = {\n",
        "    \"Llama-3-8B_addition_op1_hundredth\": 0.8,\n",
        "    \"Llama-3-8B_addition_op1_tenth\": 0.4,\n",
        "    \"Llama-3-8B_addition_op1_unit\": 0.6,\n",
        "    \"Llama-3-8B_addition_op2_hundredth\": 0.9,\n",
        "    \"Llama-3-8B_addition_op2_tenth\": 0.5,\n",
        "    \"Llama-3-8B_addition_op2_unit\": 0.6,\n",
        "    \"Llama-3-8B_subtraction_op1_hundredth\": 0.8,\n",
        "    \"Llama-3-8B_subtraction_op1_tenth\": 0.5,\n",
        "    \"Llama-3-8B_subtraction_op1_unit\": 0.6,\n",
        "    \"Llama-3-8B_subtraction_op2_hundredth\": 0.9,\n",
        "    \"Llama-3-8B_subtraction_op2_tenth\": 0.5,\n",
        "    \"Llama-3-8B_subtraction_op2_unit\": 0.6,\n",
        "    \"Olmo-2-7B_addition_op1_hundredth\": 0.8,\n",
        "    \"Olmo-2-7B_addition_op1_tenth\": 0.8,\n",
        "    \"Olmo-2-7B_addition_op1_unit\": 0.4,\n",
        "    \"Olmo-2-7B_addition_op2_hundredth\": 0.8,\n",
        "    \"Olmo-2-7B_addition_op2_tenth\": 0.8,\n",
        "    \"Olmo-2-7B_addition_op2_unit\": 0.4,\n",
        "    \"Olmo-2-7B_subtraction_op1_hundredth<\": 0.8,\n",
        "    \"Olmo-2-7B_subtraction_op1_tenth\": 0.8,\n",
        "    \"Olmo-2-7B_subtraction_op1_unit\": 0.5,\n",
        "    \"Olmo-2-7B_subtraction_op2_hundredth\": 0.9,\n",
        "    \"Olmo-2-7B_subtraction_op2_tenth\": 0.9,\n",
        "    \"Olmo-2-7B_subtraction_op2_unit\": 0.5,\n",
        "}\n",
        "\n",
        "threshold = threshold_map[key]\n",
        "\n",
        "print(layer_set)\n",
        "print(threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "new_data = defaultdict(dict)\n",
        "models = {\"Llama-3-8B\": \"meta-llama/Meta-Llama-3-8B\", \"Llama-3-70B\": \"meta-llama/Meta-Llama-3-70B\", \"Olmo-2-7B\": \"allenai/OLMo-2-1124-7B\" }\n",
        "\n",
        "for model_name in models:\n",
        "    for task in [\"addition\",\"subtraction\"]:\n",
        "        new_data[model_name][task] = defaultdict(list)\n",
        "        for operand in [\"op1\",\"op2\"]:\n",
        "            key = f\"{model_name}_{task}_{operand}\"\n",
        "            new_data[model_name][task][operand] = layer_sets[key]\n",
        "\n",
        "input_file = f\"intervention_layers_with_threholds.json\"\n",
        "with open(input_file, \"r\") as f:\n",
        "    data = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZuXzcEIRUpe"
      },
      "source": [
        "people should have the option to overwrite layer set and threshold by themselves so that they can experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx2IAdTqRZnu"
      },
      "source": [
        "Extract the MLP dimensions that have a fisher score above the chosen threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGKn-ixIRTaF"
      },
      "outputs": [],
      "source": [
        "# According to the threshold chosen, get the MLP dimensions per layer to be intervened on, i.e., the MLP neurons that belong to the digit circuit at that threshold\n",
        "\n",
        "# Open the right fisher score file\n",
        "with open(f\"Fisher_Scores/{model_name}/{task}/fisher_scores_{label}.json\", \"r\") as file:\n",
        "    fisher_scores_data = json.load(file)\n",
        "\n",
        "# Output dictionary\n",
        "layer_subspaces_map = {}\n",
        "\n",
        "# Iterate through selected layers\n",
        "for layer in layer_set:\n",
        "    key = f\"layer_{layer}\"\n",
        "    if key in fisher_scores_data:\n",
        "        values = fisher_scores_data[key]\n",
        "        indices_above_threshold = [i for i, val in enumerate(values) if val > threshold]\n",
        "        layer_subspaces_map[layer] = indices_above_threshold\n",
        "\n",
        "# Result\n",
        "print(layer_subspaces_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH3u8vtmRUWv"
      },
      "outputs": [],
      "source": [
        "################################################\n",
        "# Perform Digit-Circuit Interventions on each data point #\n",
        "################################################\n",
        "\n",
        "# Iterate through each query in the dataset\n",
        "for j, entry in tqdm(enumerate(data)):\n",
        "    data_entry = []\n",
        "    model_layers = model.config.num_hidden_layers\n",
        "    window_size = 1\n",
        "\n",
        "    sentence = entry[\"one_shot_base\"]\n",
        "    sentence_intervention = entry[\"one_shot_source\"]\n",
        "\n",
        "    base = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Number of tokens\n",
        "    tokenized_input = tokenizer(sentence, return_tensors=\"pt\", return_offsets_mapping=True)\n",
        "    input_ids = tokenized_input[\"input_ids\"].to(device)\n",
        "    num_tokens = input_ids.shape[1]\n",
        "\n",
        "    ############################\n",
        "    # Clean Run for comparison #\n",
        "    ############################\n",
        "\n",
        "    inputs = [tokenizer(sentence, return_tensors=\"pt\").to(device),]\n",
        "    res = model(**inputs[0])\n",
        "\n",
        "    distrib = res.logits\n",
        "    logits = distrib[0][-1]\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get the top 10 tokens and their probabilities\n",
        "    top_k = 50\n",
        "    top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n",
        "\n",
        "    # Convert indices to tokens\n",
        "    top_k_tokens = [tokenizer.decode(index.item()) for index in top_k_indices]\n",
        "\n",
        "    # Collect the data\n",
        "    data_temp = []\n",
        "    for token, prob in zip(top_k_tokens, top_k_probs):\n",
        "        data_temp.append({\n",
        "            \"token\": token,\n",
        "            \"prob\": prob.detach().cpu().item()\n",
        "        })\n",
        "\n",
        "    data_entry.append({\"run\": \"clean\", \"top_100\": data_temp})\n",
        "\n",
        "    ###############################################\n",
        "    # Interchange Interventions across layer sets #\n",
        "    ###############################################\n",
        "\n",
        "    # Get the index of the last token using len()\n",
        "    last_token_index = len(base['input_ids'][0]) - 1  # Use len() to get the length of the sequence\n",
        "\n",
        "    # Create intervention for specific layers\n",
        "    config = pv.IntervenableConfig([{\n",
        "        \"layer\": l,\n",
        "        \"component\": \"mlp_output\",\n",
        "        \"intervention_type\": VanillaIntervention\n",
        "        } for l in layer_set] # Pass a list instead of a single layer\n",
        "    )\n",
        "\n",
        "    pv_model = pv.IntervenableModel(config, model=model)\n",
        "\n",
        "    # Define list of subspaces based on the layer_subspaces_map\n",
        "    # Create an empty list to store the corresponding subspaces for each layer in layer_set\n",
        "    subspaces = []\n",
        "\n",
        "    # Loop over the layers in the current layer_set and fetch corresponding subspaces\n",
        "    for layer in layer_set:\n",
        "        subspaces.append(layer_subspaces_map[layer])\n",
        "\n",
        "    # run an interchange intervention\n",
        "    _, intervened_outputs = pv_model(\n",
        "      # the base input\n",
        "      base=tokenizer(sentence, return_tensors = \"pt\").to(device),\n",
        "      # the source input\n",
        "      sources=tokenizer(sentence_intervention, return_tensors = \"pt\").to(device),\n",
        "      # the location to intervene at (last token)\n",
        "      unit_locations={\"sources->base\": last_token_index},\n",
        "      subspaces = subspaces\n",
        "    )\n",
        "\n",
        "    distrib = intervened_outputs.logits\n",
        "    logits = distrib[0][-1]\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get the top 10 tokens and their probabilities\n",
        "    top_k = 100\n",
        "    top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n",
        "\n",
        "    # Convert indices to tokens\n",
        "    top_k_tokens = [tokenizer.decode(index.item()) for index in top_k_indices]\n",
        "\n",
        "    # Collect the data\n",
        "    data_temp = []\n",
        "    for token, prob in zip(top_k_tokens, top_k_probs):\n",
        "        data_temp.append({\n",
        "            \"token\": token,\n",
        "            \"prob\": prob.detach().cpu().item()\n",
        "        })\n",
        "\n",
        "    data_entry.append({\"run\": \"intervened\", \"top_100\": data_temp})\n",
        "\n",
        "    df = pd.DataFrame(data_entry)\n",
        "\n",
        "    output_dir = f\"Interventions/\"\n",
        "    os.makedirs(output_dir, exist_ok=True)  # Create the folder if it doesn't exist\n",
        "\n",
        "    df.to_csv(f\"{output_dir}/intervention_{model_name}_{task}_{operand}_{label}_threshold_{threshold}_{j}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKkH5cxIRo_f"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6r3yhnLRou_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import ast\n",
        "import re\n",
        "\n",
        "variant_labels = [\"bbb\", \"bbs\", \"bsb\", \"bss\", \"sbb\", \"sbs\", \"ssb\", \"sss\"]\n",
        "folder_path = \"Interventions/\"\n",
        "\n",
        "# Initialize accumulators: sums and counts for each run and variant label\n",
        "accumulated_probs = {\n",
        "    \"clean\": {label: 0.0 for label in variant_labels},\n",
        "    \"intervened\": {label: 0.0 for label in variant_labels}\n",
        "}\n",
        "counts = {\n",
        "    \"clean\": {label: 0 for label in variant_labels},\n",
        "    \"intervened\": {label: 0 for label in variant_labels}\n",
        "}\n",
        "\n",
        "# Get sorted CSV files\n",
        "csv_files = [f for f in os.listdir(folder_path) if re.match(r\"intervention_.*_(\\d+)\\.csv\", f)]\n",
        "csv_files.sort(key=lambda x: int(re.search(r\"_(\\d+)\", x).group(1)))\n",
        "\n",
        "for file_name in csv_files:\n",
        "    index_match = re.search(r\"_(\\d+)\\.csv$\", file_name)\n",
        "    i = int(index_match.group(1)) if index_match else None\n",
        "    if i is None:\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing example {i} from file {file_name}\")\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Map variant labels to target tokens from your loaded `data`\n",
        "    variant_token_map = {k: str(v) for k, v in data[i][\"result_variants\"].items()}\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        run_type = row[\"run\"]  # \"clean\" or \"intervened\"\n",
        "        try:\n",
        "            token_probs = ast.literal_eval(row[\"top_100\"])\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing token probs in {file_name}, run={run_type}: {e}\")\n",
        "            continue\n",
        "\n",
        "        for variant_label, target_token in variant_token_map.items():\n",
        "            for entry in token_probs:\n",
        "                if entry[\"token\"] == target_token:\n",
        "                    prob = entry[\"prob\"]\n",
        "                    accumulated_probs[run_type][variant_label] += prob\n",
        "                    counts[run_type][variant_label] += 1\n",
        "\n",
        "# Compute averages over all examples and all files\n",
        "averages = {\n",
        "    run_type: {\n",
        "        label: (accumulated_probs[run_type][label] / counts[run_type][label]) if counts[run_type][label] > 0 else 0\n",
        "        for label in variant_labels\n",
        "    }\n",
        "    for run_type in [\"clean\", \"intervened\"]\n",
        "}\n",
        "\n",
        "# Save averages to JSON\n",
        "output_file = \"average_probabilities.json\"\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(averages, f, indent=4)\n",
        "\n",
        "print(\"Averages computed and saved to\", output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBVJb4Q6RvHe"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXWGFMQXRwq2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming `averages` dict is computed as before:\n",
        "# averages = {\n",
        "#   \"clean\": {...},\n",
        "#   \"intervened\": {...}\n",
        "# }\n",
        "\n",
        "variant_labels = list(averages[\"clean\"].keys())\n",
        "\n",
        "clean_vals = [averages[\"clean\"][label] for label in variant_labels]\n",
        "intervened_vals = [averages[\"intervened\"][label] for label in variant_labels]\n",
        "diff_vals = [i - c for i, c in zip(intervened_vals, clean_vals)]\n",
        "\n",
        "x = np.arange(len(variant_labels))\n",
        "width = 0.25\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "bars1 = ax.bar(x - width, clean_vals, width, label='Clean', color='tab:blue')\n",
        "bars2 = ax.bar(x, intervened_vals, width, label='Intervened', color='tab:orange')\n",
        "bars3 = ax.bar(x + width, diff_vals, width, label='Difference (Clean - Intervened)', color='tab:green')\n",
        "\n",
        "ax.set_ylabel('Average Probability')\n",
        "ax.set_title('Average Probabilities per Variant Label')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(variant_labels)\n",
        "ax.legend()\n",
        "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttJlKtm9Ry8G"
      },
      "source": [
        "## Flip Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARa_JDyRR0Qv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import ast\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import ast\n",
        "import re\n",
        "import sys\n",
        "\n",
        "# Flip rate calculation\n",
        "expected_variant_map = {\n",
        "    \"unit\": \"bbs\",\n",
        "    \"tenth\": \"bsb\",\n",
        "    \"hundredth\": \"sbb\"\n",
        "}\n",
        "\n",
        "# Use your current label to find the expected variant\n",
        "expected_variant = expected_variant_map[label]  # 'label' must be defined in your loop or global scope\n",
        "\n",
        "flip_count = 0\n",
        "total_count = 0\n",
        "\n",
        "for file_name in csv_files:\n",
        "    index_match = re.search(r\"_(\\d+)\\.csv$\", file_name)\n",
        "    i = int(index_match.group(1)) if index_match else None\n",
        "    if i is None:\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Get token mapping for this datapoint\n",
        "    variant_token_map = {k: str(v) for k, v in data[i][\"result_variants\"].items()}\n",
        "\n",
        "    # Dictionary to store best variant per run\n",
        "    best_variant = {}\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        run_type = row[\"run\"]\n",
        "        try:\n",
        "            token_probs = ast.literal_eval(row[\"top_100\"])\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing token probs in {file_name}, run={run_type}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Find the best matching variant for this run\n",
        "        highest_prob = -1\n",
        "        predicted_variant = None\n",
        "\n",
        "        for variant_label, token in variant_token_map.items():\n",
        "            for entry in token_probs:\n",
        "                if entry[\"token\"] == token:\n",
        "                    if entry[\"prob\"] > highest_prob:\n",
        "                        highest_prob = entry[\"prob\"]\n",
        "                        predicted_variant = variant_label\n",
        "\n",
        "        best_variant[run_type] = predicted_variant\n",
        "\n",
        "    # Count flip: clean was bbb, intervened is expected variant\n",
        "    if best_variant.get(\"clean\") == \"bbb\" and best_variant.get(\"intervened\") == expected_variant:\n",
        "        flip_count += 1\n",
        "\n",
        "    total_count += 1\n",
        "\n",
        "flip_rate = flip_count / total_count if total_count > 0 else 0\n",
        "print(f\"\\n Flip Rate (from 'bbb' â†’ '{expected_variant}') = {flip_rate:.3f} over {total_count} examples.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMXIKniNXAajjJUvrCOQpTO",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
