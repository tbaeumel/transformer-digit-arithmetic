{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYUNoMbkXV+0RndAzsFX1U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbaeumel/transformer-digit-arithmetic/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the data from the repo -> anonymize!"
      ],
      "metadata": {
        "id": "NIztQfmuPlzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tbaeumel/transformer-digit-arithmetic.git"
      ],
      "metadata": {
        "id": "DeyBSXiAPlTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1j_pJjQPSh1"
      },
      "outputs": [],
      "source": [
        "%cd transformer-digit-arithmetic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get pyvene\n",
        "Olmo2 was not in pyvene - i wrote an extension so that olmo2 can be an intervenable model.\n",
        " has to be anonymized"
      ],
      "metadata": {
        "id": "Wi4rRDHKaLOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip uninstall -y pyvene  # pyvene has to be uninstalled if a standard version is currently installed\n",
        "# pip install pyvene\n",
        "!pip install git+https://github.com/tbaeumel/pyvene"
      ],
      "metadata": {
        "id": "ijicRAr8pFiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyvene as pv\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "import sys\n",
        "import torch.nn.functional as F\n",
        "from pyvene import embed_to_distrib, top_vals, format_token\n",
        "from pyvene import (\n",
        "    ZeroIntervention,\n",
        "    IntervenableModel,\n",
        "    VanillaIntervention, Intervention,\n",
        "    RepresentationConfig,\n",
        "    IntervenableConfig,\n",
        "    ConstantSourceIntervention,\n",
        "    LocalistRepresentationIntervention\n",
        ")\n",
        "from huggingface_hub import login\n",
        "import re\n",
        "import numpy\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "import transformers\n",
        "import os"
      ],
      "metadata": {
        "id": "OQSGxa7hQcKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model\n",
        "\n",
        "Llama 70B wont work on colab\n",
        "\n",
        "llama 8b and olmo 2 7b will work on l4 or a100\n",
        "\n",
        "people have to login with their huggingface key to get access to the llama models"
      ],
      "metadata": {
        "id": "93z_4vN6QivG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Huggingface to get access to model parameters\n",
        "# Paste your token here\n",
        "login('')"
      ],
      "metadata": {
        "id": "DtGz_kwgQjxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose your model\n",
        "model_name = \"Llama-3-8B\" # Llama-3-70B # Olmo-2-7B"
      ],
      "metadata": {
        "id": "_VmQMDbeQxN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\"Llama-3-8B\": \"meta-llama/Meta-Llama-3-8B\", \"Llama-3-70B\": \"meta-llama/Meta-Llama-3-70B\", \"Olmo-2-7B\": \"allenai/OLMo-2-1124-7B\" }\n",
        "\n",
        "# Set parameters\n",
        "params = {\n",
        "    'model_name': models[model_name],\n",
        "    'device': \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "}\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(params['model_name'])\n",
        "model = AutoModelForCausalLM.from_pretrained(params['model_name'], torch_dtype=torch.float16).to(params['device'])\n",
        "\n",
        "# Confirm device\n",
        "print(f\"Using device: {params['device']}\")\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  print(\"Using GPU\")"
      ],
      "metadata": {
        "id": "gCBzhTaDQx8O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}