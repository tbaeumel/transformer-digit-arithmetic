{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2cxV23OZ6dbYFXTnH2vS5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbaeumel/transformer-digit-arithmetic/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the data from the repo -> anonymize!"
      ],
      "metadata": {
        "id": "NIztQfmuPlzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tbaeumel/transformer-digit-arithmetic.git"
      ],
      "metadata": {
        "id": "DeyBSXiAPlTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1j_pJjQPSh1"
      },
      "outputs": [],
      "source": [
        "%cd transformer-digit-arithmetic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get pyvene\n",
        "Olmo2 was not in pyvene - i wrote an extension so that olmo2 can be an intervenable model.\n",
        " has to be anonymized"
      ],
      "metadata": {
        "id": "Wi4rRDHKaLOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip uninstall -y pyvene  # pyvene has to be uninstalled if a standard version is currently installed\n",
        "# pip install pyvene\n",
        "!pip install git+https://github.com/tbaeumel/pyvene"
      ],
      "metadata": {
        "id": "ijicRAr8pFiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyvene as pv\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "import sys\n",
        "import torch.nn.functional as F\n",
        "from pyvene import embed_to_distrib, top_vals, format_token\n",
        "from pyvene import (\n",
        "    ZeroIntervention,\n",
        "    IntervenableModel,\n",
        "    VanillaIntervention, Intervention,\n",
        "    RepresentationConfig,\n",
        "    IntervenableConfig,\n",
        "    ConstantSourceIntervention,\n",
        "    LocalistRepresentationIntervention\n",
        ")\n",
        "from huggingface_hub import login\n",
        "import re\n",
        "import numpy\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "import transformers\n",
        "import os"
      ],
      "metadata": {
        "id": "OQSGxa7hQcKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model\n",
        "\n",
        "Llama 70B wont work on colab\n",
        "\n",
        "llama 8b and olmo 2 7b will work on l4 or a100\n",
        "\n",
        "people have to login with their huggingface key to get access to the llama models"
      ],
      "metadata": {
        "id": "93z_4vN6QivG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Huggingface to get access to model parameters\n",
        "# Paste your token here\n",
        "login('')"
      ],
      "metadata": {
        "id": "DtGz_kwgQjxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose your model\n",
        "model_name = \"Llama-3-8B\" # Llama-3-70B # Olmo-2-7B"
      ],
      "metadata": {
        "id": "_VmQMDbeQxN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\"Llama-3-8B\": \"meta-llama/Meta-Llama-3-8B\", \"Llama-3-70B\": \"meta-llama/Meta-Llama-3-70B\", \"Olmo-2-7B\": \"allenai/OLMo-2-1124-7B\" }\n",
        "\n",
        "# Set parameters\n",
        "params = {\n",
        "    'model_name': models[model_name],\n",
        "    'device': \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "}\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(params['model_name'])\n",
        "model = AutoModelForCausalLM.from_pretrained(params['model_name'], torch_dtype=torch.float16).to(params['device'])\n",
        "\n",
        "# Confirm device\n",
        "print(f\"Using device: {params['device']}\")\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  print(\"Using GPU\")"
      ],
      "metadata": {
        "id": "gCBzhTaDQx8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Digit circuit Intervention\n",
        "\n",
        "Choose:\n",
        "- thresholds\n",
        "- digit position\n",
        "- task (operator)\n",
        "- operand (which intervention dataset)\n",
        "\n",
        "this can be changed to standard values for the thresholds and layers (see paper Section X), so that based on model and digti position the best threshold is chosen\n"
      ],
      "metadata": {
        "id": "kbk3s-9eQ8OR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# labels\n",
        "label = \"hundredth\" # \"tenth\" # \"unit\"\n",
        "# task (operator)\n",
        "task = \"addition\" # \"subtraction\"\n",
        "# which oeprand\n",
        "operand = \"op1\" # \"op2\""
      ],
      "metadata": {
        "id": "0xUTzMfLQ9tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the JSON file\n",
        "input_file = f\"Intervention_Data/intervention_data_{task}_{operand}.json\"\n",
        "with open(input_file, \"r\") as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "HwIGZ-L0RFNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "all of this should be stored in some sort of data look up file"
      ],
      "metadata": {
        "id": "wubB5XCNRHBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# layer sets to intervene on based on model, task, and operand\n",
        "# TODO do this elegantly\n",
        "\n",
        "key = f\"{model_name}_{task}_{operand}\"\n",
        "\n",
        "layer_sets = {\n",
        "    \"Llama-3-8B_addition_op1\": [16, 17, 18, 19, 20, 21, 22, 23, 24],\n",
        "    \"Llama-3-8B_addition_op2\": [15, 16, 17, 18, 19, 20, 21, 22, 23, 24],\n",
        "    \"Llama-3-8B_subtraction_op1\": [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28],\n",
        "    \"Llama-3-8B_subtraction_op2\": [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28],\n",
        "    \"Llama-3-70B_addition_op1\": [39, ..., 56],\n",
        "    \"Llama-3-70B_addition_op2\": [39, ..., 56],\n",
        "    \"Llama-3-70B_subtraction_op1\": [39, ..., 58],\n",
        "    \"Llama-3-70B_subtraction_op2\": [39, ..., 58],\n",
        "    \"Olmo-2-7B_addition_op1\": [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\n",
        "    \"Olmo-2-7B_addition_op2\": [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\n",
        "    \"Olmo-2-7B_subtraction_op1\": [19, 20, 21, 22, 23, 24, 25, 26, 27],\n",
        "    \"Olmo-2-7B_subtraction_op2\": [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27],\n",
        "    }\n",
        "\n",
        "layer_set = layer_sets[key]\n",
        "\n",
        "# TODO add llama 70 when results are there (best threshold)\n",
        "key = key+\"_\"+label\n",
        "threshold_map = {\n",
        "    \"Llama-3-8B_addition_op1_hundredth\": 0.8,\n",
        "    \"Llama-3-8B_addition_op1_tenth\": 0.4,\n",
        "    \"Llama-3-8B_addition_op1_unit\": 0.6,\n",
        "    \"Llama-3-8B_addition_op2_hundredth\": 0.9,\n",
        "    \"Llama-3-8B_addition_op2_tenth\": 0.5,\n",
        "    \"Llama-3-8B_addition_op2_unit\": 0.6,\n",
        "    \"Llama-3-8B_subtraction_op1_hundredth\": 0.8,\n",
        "    \"Llama-3-8B_subtraction_op1_tenth\": 0.5,\n",
        "    \"Llama-3-8B_subtraction_op1_unit\": 0.6,\n",
        "    \"Llama-3-8B_subtraction_op2_hundredth\": 0.9,\n",
        "    \"Llama-3-8B_subtraction_op2_tenth\": 0.5,\n",
        "    \"Llama-3-8B_subtraction_op2_unit\": 0.6,\n",
        "    \"Olmo-2-7B_addition_op1_hundredth\": 0.8,\n",
        "    \"Olmo-2-7B_addition_op1_tenth\": 0.8,\n",
        "    \"Olmo-2-7B_addition_op1_unit\": 0.4,\n",
        "    \"Olmo-2-7B_addition_op2_hundredth\": 0.8,\n",
        "    \"Olmo-2-7B_addition_op2_tenth\": 0.8,\n",
        "    \"Olmo-2-7B_addition_op2_unit\": 0.4,\n",
        "    \"Olmo-2-7B_subtraction_op1_hundredth<\": 0.8,\n",
        "    \"Olmo-2-7B_subtraction_op1_tenth\": 0.8,\n",
        "    \"Olmo-2-7B_subtraction_op1_unit\": 0.5,\n",
        "    \"Olmo-2-7B_subtraction_op2_hundredth\": 0.9,\n",
        "    \"Olmo-2-7B_subtraction_op2_tenth\": 0.9,\n",
        "    \"Olmo-2-7B_subtraction_op2_unit\": 0.5,\n",
        "}\n",
        "\n",
        "threshold = threshold_map[key]\n",
        "\n",
        "print(layer_set)\n",
        "print(threshold)"
      ],
      "metadata": {
        "id": "wqRyI89ZRQ2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "people should have the option to overwrite layer set and threshold by themselves so that they can experiment"
      ],
      "metadata": {
        "id": "eZuXzcEIRUpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the MLP dimensions that have a fisher score above the chosen threshold"
      ],
      "metadata": {
        "id": "Kx2IAdTqRZnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# According to the threshold chosen, get the MLP dimensions per layer to be intervened on, i.e., the MLP neurons that belong to the digit circuit at that threshold\n",
        "\n",
        "# Open the right fisher score file\n",
        "with open(f\"Fisher_Scores/{model_name}/{task}/fisher_scores_{label}.json\", \"r\") as file:\n",
        "    fisher_scores_data = json.load(file)\n",
        "\n",
        "# Output dictionary\n",
        "layer_subspaces_map = {}\n",
        "\n",
        "# Iterate through selected layers\n",
        "for layer in layer_set:\n",
        "    key = f\"layer_{layer}\"\n",
        "    if key in fisher_scores_data:\n",
        "        values = fisher_scores_data[key]\n",
        "        indices_above_threshold = [i for i, val in enumerate(values) if val > threshold]\n",
        "        layer_subspaces_map[layer] = indices_above_threshold\n",
        "\n",
        "# Result\n",
        "print(layer_subspaces_map)"
      ],
      "metadata": {
        "id": "xGKn-ixIRTaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################\n",
        "# Perform Digit-Circuit Interventions on each data point #\n",
        "################################################\n",
        "\n",
        "# Iterate through each query in the dataset\n",
        "for j, entry in tqdm(enumerate(data)):\n",
        "    data_entry = []\n",
        "    model_layers = model.config.num_hidden_layers\n",
        "    window_size = 1\n",
        "\n",
        "    sentence = entry[\"one_shot_base\"]\n",
        "    sentence_intervention = entry[\"one_shot_source\"]\n",
        "\n",
        "    base = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Number of tokens\n",
        "    tokenized_input = tokenizer(sentence, return_tensors=\"pt\", return_offsets_mapping=True)\n",
        "    input_ids = tokenized_input[\"input_ids\"].to(device)\n",
        "    num_tokens = input_ids.shape[1]\n",
        "\n",
        "    ############################\n",
        "    # Clean Run for comparison #\n",
        "    ############################\n",
        "\n",
        "    inputs = [tokenizer(sentence, return_tensors=\"pt\").to(device),]\n",
        "    res = model(**inputs[0])\n",
        "\n",
        "    distrib = res.logits\n",
        "    logits = distrib[0][-1]\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get the top 10 tokens and their probabilities\n",
        "    top_k = 50\n",
        "    top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n",
        "\n",
        "    # Convert indices to tokens\n",
        "    top_k_tokens = [tokenizer.decode(index.item()) for index in top_k_indices]\n",
        "\n",
        "    # Collect the data\n",
        "    data_temp = []\n",
        "    for token, prob in zip(top_k_tokens, top_k_probs):\n",
        "        data_temp.append({\n",
        "            \"token\": token,\n",
        "            \"prob\": prob.detach().cpu().item()\n",
        "        })\n",
        "\n",
        "    data_entry.append({\"run\": \"clean\", \"top_100\": data_temp})\n",
        "\n",
        "    ###############################################\n",
        "    # Interchange Interventions across layer sets #\n",
        "    ###############################################\n",
        "\n",
        "    # Get the index of the last token using len()\n",
        "    last_token_index = len(base['input_ids'][0]) - 1  # Use len() to get the length of the sequence\n",
        "\n",
        "    # Create intervention for specific layers\n",
        "    config = pv.IntervenableConfig([{\n",
        "        \"layer\": l,\n",
        "        \"component\": \"mlp_output\",\n",
        "        \"intervention_type\": VanillaIntervention\n",
        "        } for l in layer_set] # Pass a list instead of a single layer\n",
        "    )\n",
        "\n",
        "    pv_model = pv.IntervenableModel(config, model=model)\n",
        "\n",
        "    # Define list of subspaces based on the layer_subspaces_map\n",
        "    # Create an empty list to store the corresponding subspaces for each layer in layer_set\n",
        "    subspaces = []\n",
        "\n",
        "    # Loop over the layers in the current layer_set and fetch corresponding subspaces\n",
        "    for layer in layer_set:\n",
        "        subspaces.append(layer_subspaces_map[layer])\n",
        "\n",
        "    # run an interchange intervention\n",
        "    _, intervened_outputs = pv_model(\n",
        "      # the base input\n",
        "      base=tokenizer(sentence, return_tensors = \"pt\").to(device),\n",
        "      # the source input\n",
        "      sources=tokenizer(sentence_intervention, return_tensors = \"pt\").to(device),\n",
        "      # the location to intervene at (last token)\n",
        "      unit_locations={\"sources->base\": last_token_index},\n",
        "      subspaces = subspaces\n",
        "    )\n",
        "\n",
        "    distrib = intervened_outputs.logits\n",
        "    logits = distrib[0][-1]\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get the top 10 tokens and their probabilities\n",
        "    top_k = 100\n",
        "    top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n",
        "\n",
        "    # Convert indices to tokens\n",
        "    top_k_tokens = [tokenizer.decode(index.item()) for index in top_k_indices]\n",
        "\n",
        "    # Collect the data\n",
        "    data_temp = []\n",
        "    for token, prob in zip(top_k_tokens, top_k_probs):\n",
        "        data_temp.append({\n",
        "            \"token\": token,\n",
        "            \"prob\": prob.detach().cpu().item()\n",
        "        })\n",
        "\n",
        "    data_entry.append({\"run\": \"intervened\", \"top_100\": data_temp})\n",
        "\n",
        "    df = pd.DataFrame(data_entry)\n",
        "\n",
        "    output_dir = f\"Interventions/\"\n",
        "    os.makedirs(output_dir, exist_ok=True)  # Create the folder if it doesn't exist\n",
        "\n",
        "    df.to_csv(f\"{output_dir}/intervention_{model_name}_{task}_{operand}_{label}_threshold_{threshold}_{j}.csv\")"
      ],
      "metadata": {
        "id": "AH3u8vtmRUWv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}