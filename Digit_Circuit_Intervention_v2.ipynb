{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIztQfmuPlzv"
      },
      "source": [
        "Get pyvene\n",
        "\n",
        "Olmo2 was not in pyvene - i wrote an extension so that olmo2 can be an intervenable model.\n",
        " -> anonymized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip uninstall -y pyvene  # pyvene has to be uninstalled if a standard version is currently installed\n",
        "# pip install pyvene\n",
        "%cd /content/\n",
        "!wget https://anonymous.4open.science/api/repo/pyvene_Olmo2/zip -O lib.zip\n",
        "!unzip lib.zip -d pyvene_Olmo2\n",
        "%cd pyvene_Olmo2\n",
        "!pip install -e .\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/pyvene_Olmo2')  # Adjust as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the data from the repo -> anonymize!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeyBSXiAPlTn",
        "outputId": "03e4105e-ac4f-4689-bd7f-228d715deea4"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "!wget https://anonymous.4open.science/api/repo/tda-C722/zip -O code.zip\n",
        "!unzip code.zip -d transformer-digit-arithmetic\n",
        "%cd transformer-digit-arithmetic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHeK7njlbLwe",
        "outputId": "e6268c0e-0398-4624-9f96-7ec5a7b9e005"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1j_pJjQPSh1",
        "outputId": "0fc1d8ff-c3d0-4079-de22-5382b2164b91"
      },
      "outputs": [],
      "source": [
        "%cd transformer-digit-arithmetic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQSGxa7hQcKu",
        "outputId": "3c1eaf38-2d37-4e79-83b2-c171d120fe65"
      },
      "outputs": [],
      "source": [
        "import pyvene as pv\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "import sys\n",
        "import torch.nn.functional as F\n",
        "from pyvene import embed_to_distrib, top_vals, format_token\n",
        "from pyvene import (\n",
        "    ZeroIntervention,\n",
        "    IntervenableModel,\n",
        "    VanillaIntervention, Intervention,\n",
        "    RepresentationConfig,\n",
        "    IntervenableConfig,\n",
        "    ConstantSourceIntervention,\n",
        "    LocalistRepresentationIntervention\n",
        ")\n",
        "from huggingface_hub import login\n",
        "import re\n",
        "import numpy\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "import transformers\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93z_4vN6QivG"
      },
      "source": [
        "## Load model\n",
        "\n",
        "Llama 70B wont work on colab\n",
        "\n",
        "llama 8b and olmo 2 7b will work on l4 or a100\n",
        "\n",
        "people have to login with their huggingface key to get access to the llama models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtGz_kwgQjxm"
      },
      "outputs": [],
      "source": [
        "# Login to Huggingface to get access to model parameters\n",
        "# Paste your token here\n",
        "HugginFace_Token = \"<replace_with_your_token>\"\n",
        "login(HugginFace_Token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VmQMDbeQxN2"
      },
      "outputs": [],
      "source": [
        "# choose your model\n",
        "model_name = \"Llama-3-8B\" # Llama-3-70B # Olmo-2-7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "ed08da4944744403baedc3abacf26a69",
            "0e6c85bb6afa446e8c22b9bdecf9ebac",
            "1fd8359385d4473b9249736a61dbc9ef",
            "4af381af35af4fe1a99c1a827ce069ef",
            "e77b709fd4c04e1aa8162558af5b40fa",
            "6cd96f2861df485c98963cee8fd11f9f",
            "294edc2636974381973a4981ca576e96",
            "34a5cea0e4f344ee999db2d8b94023ae",
            "67b063795012434ca84fda35af94a22f",
            "11644d5a8dbb4bd8a17cee2967f09c44",
            "c69045c248594ea594850243be21de93"
          ]
        },
        "id": "gCBzhTaDQx8O",
        "outputId": "2452d127-9160-4afb-dc3f-bc6f1530a043"
      },
      "outputs": [],
      "source": [
        "models = {\"Llama-3-8B\": \"meta-llama/Meta-Llama-3-8B\", \"Llama-3-70B\": \"meta-llama/Meta-Llama-3-70B\", \"Olmo-2-7B\": \"allenai/OLMo-2-1124-7B\" }\n",
        "\n",
        "# Set parameters\n",
        "params = {\n",
        "    'model_name': models[model_name],\n",
        "    'device': \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "}\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(params['model_name'])\n",
        "model = AutoModelForCausalLM.from_pretrained(params['model_name'], torch_dtype=torch.float16).to(params['device'])\n",
        "\n",
        "# Confirm device\n",
        "print(f\"Using device: {params['device']}\")\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  print(\"Using GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbk3s-9eQ8OR"
      },
      "source": [
        "# Digit circuit Intervention\n",
        "\n",
        "Choose:\n",
        "- thresholds (pre computed using Fisher scores)\n",
        "- digit position (hundredth,tenth,unit)\n",
        "- task (operator) (+,-)\n",
        "- operand (which intervention dataset) (op1 + op2 = ..) two variation either fix op1 or op2\n",
        "\n",
        "this can be changed to standard values for the thresholds and layers (see paper Section X), so that based on model and digti position the best threshold is chosen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xUTzMfLQ9tP"
      },
      "outputs": [],
      "source": [
        "# labels\n",
        "label = \"hundredth\" # \"tenth\" # \"unit\"\n",
        "# task (operator)\n",
        "task = \"addition\" # \"subtraction\"\n",
        "# which oeprand\n",
        "operand = \"op1\" # \"op2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lGuiGwxufMS",
        "outputId": "ff25d60f-7f6f-4f30-f825-2daf39ff0683"
      },
      "outputs": [],
      "source": [
        "# Load the JSON file\n",
        "input_file = f\"Intervention_Data/intervention_data_{task}_{operand}.json\"\n",
        "with open(input_file, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# layer sets to intervene on based on model, task, and operand\n",
        "# TODO do this elegantly\n",
        "\n",
        "with open(f\"intervene_layers.json\", \"r\") as f:\n",
        "    layer_sets = json.load(f)\n",
        "layer_set = layer_sets[model_name][task][operand] ## return layers set for the corresponding experiment\n",
        "\n",
        "\n",
        "with open(f\"fisher_scores_threshold_map.json\", \"r\") as f:\n",
        "    threshold_map = json.load(f)\n",
        "threshold = threshold_map[model_name][task][operand][label] ## return threshold float for the corresponding experiment\n",
        "\n",
        "print(\"layer_set:\",layer_set)\n",
        "print(\"threshold:\",threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx2IAdTqRZnu"
      },
      "source": [
        "Extract the MLP dimensions that have a fisher score above the chosen threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGKn-ixIRTaF",
        "outputId": "360d0637-8500-4a5a-bb5d-bb53e047089b"
      },
      "outputs": [],
      "source": [
        "# According to the threshold chosen, get the MLP dimensions per layer to be intervened on, i.e., the MLP neurons that belong to the digit circuit at that threshold\n",
        "\n",
        "# Open the right fisher score file\n",
        "with open(f\"Fisher_Scores/{model_name}/{task}/fisher_scores_{label}.json\", \"r\") as file:\n",
        "    fisher_scores_data = json.load(file) # return a victor of Fisher scores for each latent in layer_i. example: for llama3-8b (layer_16: sahpe(4096), layer_17:...)\n",
        "\n",
        "\n",
        "# Output dictionary\n",
        "layer_subspaces_map = {}\n",
        "\n",
        "# Iterate through best selected layers\n",
        "layers = layer_set # you can choose a subset of layers to test output [16,17]\n",
        "min_thresh = threshold # you can choose different threshold 0.8\n",
        "for layer in layers:\n",
        "    key = f\"layer_{layer}\"\n",
        "    if key in fisher_scores_data:\n",
        "        values = fisher_scores_data[key]\n",
        "        indices_above_threshold = [i for i, val in enumerate(values) if val > min_thresh] ### keep neurons indices that pass the a threshold\n",
        "        layer_subspaces_map[layer] = indices_above_threshold\n",
        "\n",
        "# Result\n",
        "print(layer_subspaces_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH3u8vtmRUWv",
        "outputId": "6a160071-0d53-49f9-af27-32d2c5d1acdb"
      },
      "outputs": [],
      "source": [
        "################################################\n",
        "# Perform Digit-Circuit Interventions on each data point #\n",
        "################################################\n",
        "\n",
        "# Iterate through each query in the dataset\n",
        "for j, entry in tqdm(enumerate(data)):\n",
        "    data_entry = []\n",
        "    model_layers = model.config.num_hidden_layers\n",
        "    window_size = 1\n",
        "\n",
        "    sentence = entry[\"one_shot_base\"]\n",
        "    sentence_intervention = entry[\"one_shot_source\"]\n",
        "\n",
        "    base = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Number of tokens\n",
        "    tokenized_input = tokenizer(sentence, return_tensors=\"pt\", return_offsets_mapping=True)\n",
        "    input_ids = tokenized_input[\"input_ids\"].to(device)\n",
        "    num_tokens = input_ids.shape[1]\n",
        "\n",
        "    ############################\n",
        "    # Clean Run for comparison #\n",
        "    ############################\n",
        "\n",
        "    inputs = [tokenizer(sentence, return_tensors=\"pt\").to(device),]\n",
        "    res = model(**inputs[0])\n",
        "\n",
        "    distrib = res.logits\n",
        "    logits = distrib[0][-1]\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get the top 10 tokens and their probabilities\n",
        "    top_k = 50\n",
        "    top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n",
        "\n",
        "    # Convert indices to tokens\n",
        "    top_k_tokens = [tokenizer.decode(index.item()) for index in top_k_indices]\n",
        "\n",
        "    # Collect the data\n",
        "    data_temp = []\n",
        "    for token, prob in zip(top_k_tokens, top_k_probs):\n",
        "        data_temp.append({\n",
        "            \"token\": token,\n",
        "            \"prob\": prob.detach().cpu().item()\n",
        "        })\n",
        "\n",
        "    data_entry.append({\"run\": \"clean\", \"top_100\": data_temp})\n",
        "\n",
        "    ###############################################\n",
        "    # Interchange Interventions across layer sets #\n",
        "    ###############################################\n",
        "\n",
        "    # Get the index of the last token using len()\n",
        "    last_token_index = len(base['input_ids'][0]) - 1  # Use len() to get the length of the sequence\n",
        "\n",
        "    # Create intervention for specific layers\n",
        "    config = pv.IntervenableConfig([{\n",
        "        \"layer\": l,\n",
        "        \"component\": \"mlp_output\",\n",
        "        \"intervention_type\": VanillaIntervention\n",
        "        } for l in layer_set] # Pass a list instead of a single layer\n",
        "    )\n",
        "\n",
        "    pv_model = pv.IntervenableModel(config, model=model)\n",
        "\n",
        "    # Define list of subspaces based on the layer_subspaces_map\n",
        "    # Create an empty list to store the corresponding subspaces for each layer in layer_set\n",
        "    subspaces = []\n",
        "\n",
        "    # Loop over the layers in the current layer_set and fetch corresponding subspaces\n",
        "    for layer in layer_set:\n",
        "        subspaces.append(layer_subspaces_map[layer])\n",
        "\n",
        "    # run an interchange intervention\n",
        "    _, intervened_outputs = pv_model(\n",
        "      # the base input\n",
        "      base=tokenizer(sentence, return_tensors = \"pt\").to(device),\n",
        "      # the source input\n",
        "      sources=tokenizer(sentence_intervention, return_tensors = \"pt\").to(device),\n",
        "      # the location to intervene at (last token)\n",
        "      unit_locations={\"sources->base\": last_token_index},\n",
        "      subspaces = subspaces\n",
        "    )\n",
        "\n",
        "    distrib = intervened_outputs.logits\n",
        "    logits = distrib[0][-1]\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get the top 10 tokens and their probabilities\n",
        "    top_k = 100\n",
        "    top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n",
        "\n",
        "    # Convert indices to tokens\n",
        "    top_k_tokens = [tokenizer.decode(index.item()) for index in top_k_indices]\n",
        "\n",
        "    # Collect the data\n",
        "    data_temp = []\n",
        "    for token, prob in zip(top_k_tokens, top_k_probs):\n",
        "        data_temp.append({\n",
        "            \"token\": token,\n",
        "            \"prob\": prob.detach().cpu().item()\n",
        "        })\n",
        "\n",
        "    data_entry.append({\"run\": \"intervened\", \"top_100\": data_temp})\n",
        "\n",
        "    df = pd.DataFrame(data_entry)\n",
        "\n",
        "    output_dir = f\"Interventions/\"\n",
        "    os.makedirs(output_dir, exist_ok=True)  # Create the folder if it doesn't exist\n",
        "\n",
        "    df.to_csv(f\"{output_dir}/intervention_{model_name}_{task}_{operand}_{label}_threshold_{threshold}_{j}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKkH5cxIRo_f"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6r3yhnLRou_",
        "outputId": "cec0fd06-36fe-495f-e539-1cc671f9c1e7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import ast\n",
        "import re\n",
        "\n",
        "variant_labels = [\"bbb\", \"bbs\", \"bsb\", \"bss\", \"sbb\", \"sbs\", \"ssb\", \"sss\"]\n",
        "folder_path = \"Interventions/\"\n",
        "\n",
        "# Initialize accumulators: sums and counts for each run and variant label\n",
        "accumulated_probs = {\n",
        "    \"clean\": {label: 0.0 for label in variant_labels},\n",
        "    \"intervened\": {label: 0.0 for label in variant_labels}\n",
        "}\n",
        "counts = {\n",
        "    \"clean\": {label: 0 for label in variant_labels},\n",
        "    \"intervened\": {label: 0 for label in variant_labels}\n",
        "}\n",
        "\n",
        "# Get sorted CSV files\n",
        "csv_files = [f for f in os.listdir(folder_path) if re.match(r\"intervention_.*_(\\d+)\\.csv\", f)]\n",
        "csv_files.sort(key=lambda x: int(re.search(r\"_(\\d+)\", x).group(1)))\n",
        "\n",
        "for file_name in csv_files:\n",
        "    index_match = re.search(r\"_(\\d+)\\.csv$\", file_name)\n",
        "    i = int(index_match.group(1)) if index_match else None\n",
        "    if i is None:\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing example {i} from file {file_name}\")\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Map variant labels to target tokens from your loaded `data`\n",
        "    variant_token_map = {k: str(v) for k, v in data[i][\"result_variants\"].items()}\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        run_type = row[\"run\"]  # \"clean\" or \"intervened\"\n",
        "        try:\n",
        "            token_probs = ast.literal_eval(row[\"top_100\"])\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing token probs in {file_name}, run={run_type}: {e}\")\n",
        "            continue\n",
        "\n",
        "        for variant_label, target_token in variant_token_map.items():\n",
        "            for entry in token_probs:\n",
        "                if entry[\"token\"] == target_token:\n",
        "                    prob = entry[\"prob\"]\n",
        "                    accumulated_probs[run_type][variant_label] += prob\n",
        "                    counts[run_type][variant_label] += 1\n",
        "\n",
        "# Compute averages over all examples and all files\n",
        "averages = {\n",
        "    run_type: {\n",
        "        label: (accumulated_probs[run_type][label] / counts[run_type][label]) if counts[run_type][label] > 0 else 0\n",
        "        for label in variant_labels\n",
        "    }\n",
        "    for run_type in [\"clean\", \"intervened\"]\n",
        "}\n",
        "\n",
        "# Save averages to JSON\n",
        "output_file = \"average_probabilities.json\"\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(averages, f, indent=4)\n",
        "\n",
        "print(\"Averages computed and saved to\", output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBVJb4Q6RvHe"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "vXWGFMQXRwq2",
        "outputId": "8b546b1c-b903-4df7-bd2d-fc8dc0e8b610"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming `averages` dict is computed as before:\n",
        "# averages = {\n",
        "#   \"clean\": {...},\n",
        "#   \"intervened\": {...}\n",
        "# }\n",
        "\n",
        "variant_labels = list(averages[\"clean\"].keys())\n",
        "\n",
        "clean_vals = [averages[\"clean\"][label] for label in variant_labels]\n",
        "intervened_vals = [averages[\"intervened\"][label] for label in variant_labels]\n",
        "diff_vals = [i - c for i, c in zip(intervened_vals, clean_vals)]\n",
        "\n",
        "x = np.arange(len(variant_labels))\n",
        "width = 0.25\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "bars1 = ax.bar(x - width, clean_vals, width, label='Clean', color='tab:blue')\n",
        "bars2 = ax.bar(x, intervened_vals, width, label='Intervened', color='tab:orange')\n",
        "bars3 = ax.bar(x + width, diff_vals, width, label='Difference (Clean - Intervened)', color='tab:green')\n",
        "\n",
        "ax.set_ylabel('Average Probability')\n",
        "ax.set_title('Average Probabilities per Variant Label')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(variant_labels)\n",
        "ax.legend()\n",
        "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttJlKtm9Ry8G"
      },
      "source": [
        "## Flip Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARa_JDyRR0Qv",
        "outputId": "dab330fb-6966-4f43-ffa6-c03c7e1fae13"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import ast\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import ast\n",
        "import re\n",
        "import sys\n",
        "\n",
        "# Flip rate calculation\n",
        "expected_variant_map = {\n",
        "    \"unit\": \"bbs\",\n",
        "    \"tenth\": \"bsb\",\n",
        "    \"hundredth\": \"sbb\"\n",
        "}\n",
        "\n",
        "# Use your current label to find the expected variant\n",
        "expected_variant = expected_variant_map[label]  # 'label' must be defined in your loop or global scope\n",
        "\n",
        "flip_count = 0\n",
        "total_count = 0\n",
        "\n",
        "for file_name in csv_files:\n",
        "    index_match = re.search(r\"_(\\d+)\\.csv$\", file_name)\n",
        "    i = int(index_match.group(1)) if index_match else None\n",
        "    if i is None:\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Get token mapping for this datapoint\n",
        "    variant_token_map = {k: str(v) for k, v in data[i][\"result_variants\"].items()}\n",
        "\n",
        "    # Dictionary to store best variant per run\n",
        "    best_variant = {}\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        run_type = row[\"run\"]\n",
        "        try:\n",
        "            token_probs = ast.literal_eval(row[\"top_100\"])\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing token probs in {file_name}, run={run_type}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Find the best matching variant for this run\n",
        "        highest_prob = -1\n",
        "        predicted_variant = None\n",
        "\n",
        "        for variant_label, token in variant_token_map.items():\n",
        "            for entry in token_probs:\n",
        "                if entry[\"token\"] == token:\n",
        "                    if entry[\"prob\"] > highest_prob:\n",
        "                        highest_prob = entry[\"prob\"]\n",
        "                        predicted_variant = variant_label\n",
        "\n",
        "        best_variant[run_type] = predicted_variant\n",
        "\n",
        "    # Count flip: clean was bbb, intervened is expected variant\n",
        "    if best_variant.get(\"clean\") == \"bbb\" and best_variant.get(\"intervened\") == expected_variant:\n",
        "        flip_count += 1\n",
        "\n",
        "    total_count += 1\n",
        "\n",
        "flip_rate = flip_count / total_count if total_count > 0 else 0\n",
        "print(f\"\\n Flip Rate (from 'bbb' → '{expected_variant}') = {flip_rate:.3f} over {total_count} examples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eq017nNDx9XV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e6c85bb6afa446e8c22b9bdecf9ebac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_6cd96f2861df485c98963cee8fd11f9f",
            "placeholder": "​",
            "style": "IPY_MODEL_294edc2636974381973a4981ca576e96",
            "tabbable": null,
            "tooltip": null,
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "11644d5a8dbb4bd8a17cee2967f09c44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fd8359385d4473b9249736a61dbc9ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_34a5cea0e4f344ee999db2d8b94023ae",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_67b063795012434ca84fda35af94a22f",
            "tabbable": null,
            "tooltip": null,
            "value": 4
          }
        },
        "294edc2636974381973a4981ca576e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "34a5cea0e4f344ee999db2d8b94023ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4af381af35af4fe1a99c1a827ce069ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_11644d5a8dbb4bd8a17cee2967f09c44",
            "placeholder": "​",
            "style": "IPY_MODEL_c69045c248594ea594850243be21de93",
            "tabbable": null,
            "tooltip": null,
            "value": " 4/4 [00:03&lt;00:00,  1.27it/s]"
          }
        },
        "67b063795012434ca84fda35af94a22f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cd96f2861df485c98963cee8fd11f9f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c69045c248594ea594850243be21de93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "e77b709fd4c04e1aa8162558af5b40fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed08da4944744403baedc3abacf26a69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e6c85bb6afa446e8c22b9bdecf9ebac",
              "IPY_MODEL_1fd8359385d4473b9249736a61dbc9ef",
              "IPY_MODEL_4af381af35af4fe1a99c1a827ce069ef"
            ],
            "layout": "IPY_MODEL_e77b709fd4c04e1aa8162558af5b40fa",
            "tabbable": null,
            "tooltip": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
